<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Zhipeng Yin </title> <meta name="author" content="Zhipeng Yin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?v=968f18c1b5989e1d13fe857a1ab464cb"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sssstefan.github.io//publications/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> Zhipeng Yin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Talk </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>You can also find my papers on <a href="https://scholar.google.com/citations?user=1nVpMXgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">my Google Scholar profile</a>.</p> <script src="/assets/js/bibsearch.js?v=1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <div class="row"> <div id="yin2026fairness" class="col-sm-10"> <div class="title">Fairness Definitions in Language Models Explained</div> <div class="author"> <em>Zhipeng Yin</em>, Zichong Wang, Avash Palikhe, and Wenbin Zhang </div> <div class="periodical"> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.70063" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/stefan0711/Fairness-in-Large-Language-Models/tree/main/definitions" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their transformer architecture: encoder-only, decoder-only, and encoder-decoder LMs. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The repository is publicly available online at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div id="wang2025unified" class="col-sm-10"> <div class="title">A unified framework for fair graph generation: Theoretical guarantees and empirical advances</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, and Wenbin Zhang </div> <div class="periodical"> <em>The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=T85ADT8a2y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Graph generation models play pivotal roles in many real-world applications, from data augmentation to privacy-preserving. Despite their deployment successes, existing approaches often exhibit fairness issues, limiting their adoption in high-risk decision-making applications. Most existing fair graph generation works are based on autoregressive models that suffer from ordering sensitivity, while primarily addressing structural bias and overlooking the critical issue of feature bias. To this end, we propose FairGEM, a novel one-shot graph generation framework designed to mitigate both graph structural bias and node feature bias simultaneously. Furthermore, our theoretical analysis establishes that FairGEM delivers substantially stronger fairness guarantees than existing models while preserving generation quality. Extensive experiments across multiple real-world datasets demonstrate that FairGEM achieves superior performance in both generation quality and fairness.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="amon2025uncertain" class="col-sm-10"> <div class="title">Uncertain boundaries: Multidisciplinary approaches to copyright issues in generative ai</div> <div class="author"> Archer Amon, <em>Zhipeng Yin</em>, Zichong Wang, Avash Palikhe, Tongjia Yu, and Wenbin Zhang </div> <div class="periodical"> <em>ACM SIGKDD Explorations Newsletter</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2404.08221" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Generative AI is becoming increasingly prevalent in creative f ields, sparking urgent debates over how current copyright laws can keep pace with technological innovation. Recent controversies of AI models generating near-replicas of copyrighted material highlight the need to adapt current legal frameworks and develop technical methods to mitigate copyright infringement risks. This task requires understanding the intersection between computational concepts such as large-scale data scraping and probabilistic content generation, legal definitions of originality and fair use, and economic impacts on intellectual property (IP) rights holders. However, most existing research on copyright in AI takes a purely computer science or law-based approach, leaving a gap in coordinating these approaches that only multidisciplinary efforts can effectively address. To bridge this gap, our survey adopts a comprehensive approach synthesizing insights from law, policy, economics, and computer science. It begins by discussing the foundational goals and considerations that should be applied to copyright in generative AI, followed by methods for detecting and assessing potential violations in AI system outputs. Next, it explores various regulatory options influenced by legal, policy, and economic frameworks to manage and mitigate copyright concerns associated with generative AI and reconcile the interests of IP rights holders with that of generative AI producers. The discussion then introduces techniques to safeguard individual creative works from unauthorized replication, such as watermarking and cryptographic protections. Finally, it describes advanced training strategies designed to prevent AI models from reproducing protected content. In doing so, we highlight key opportunities for action and offer actionable strategies that creators, developers, and policymakers can use in navigating the evolving copyright landscape.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yin2025amcr" class="col-sm-10"> <div class="title">AMCR: A framework for assessing and mitigating copyright risks in generative models</div> <div class="author"> <em>Zhipeng Yin</em>, Zichong Wang, Avash Palikhe, Zhen Liu, Jun Liu, and Wenbin Zhang </div> <div class="periodical"> <em>28th European Conference on Artificial Intelligence (ECAI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2509.00641" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025ai" class="col-sm-10"> <div class="title">AI fairness beyond complete demographics: Current achievements and future directions</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Roland HC Yap, and Wenbin Zhang </div> <div class="periodical"> <em>28th European Conference on Artificial Intelligence (ECAI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2511.13525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025towards" class="col-sm-10"> <div class="title">Towards fair graph-based machine learning software: unveiling and mitigating graph model bias</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Xingyu Zhang, Yuying Zhang, Xudong He, Shaowei Wang, Houbing Song, Liping Yang, and Wenbin Zhang </div> <div class="periodical"> <em>AI and Ethics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s43681-025-00756-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Machine Learning (ML) software is increasingly influencing decisions that impact individualsâ€™ lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social groups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works focus on tabular data. Exploration in the context of graph data, which is widely prevalent in real-world applications, has been relatively neglected. This paper thus sheds light on the impact of biases within graph data on the fairness of ML models and how to detect biases in graph based software. Subsequently, we introduce Fair Graph Redistribution and Generation (FGRG), a novel fair imbalance node learning technique designed to enhance the fairness of graph-based ML software. Specifically, FGRG commences with the generation of unbiased node embeddings for each node. Next, FGRG identifies node similarities within the embedding space and generates new nodes to rebalance the internal distribution, ensuring subgroups with different sensitive attributes are balanced representations across both positive and negative classes. Experiments on 8 real-world graphs across 3 fairness and 4 performance measurements show that our framework significantly outperforms the state-of-the-art baselines, and also achieves comparable prediction performance. In particular, FGRG beats the trade-off baseline in 79.16% of the fairness cases evaluated and 62.5% of the performance cases evaluated.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025fairness" class="col-sm-10"> <div class="title">Fairness-aware graph representation learning with limited demographic information</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Liping Yang, Jun Zhuang, Rui Yu, Qingzhao Kong, and Wenbin Zhang </div> <div class="periodical"> <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" style="background:#ffb9b9" role="button">Best student paper award</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-032-05962-8_21" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ Best student paper award</p> </div> <div class="abstract hidden"> <p>Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each nodeâ€™s contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the frameworkâ€™s effectiveness in both mitigating bias and maintaining model utility.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025redefining" class="col-sm-10"> <div class="title">Redefining fairness: A multi-dimensional perspective and integrated evaluation framework</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Zhen Liu, Roland HC Yap, Xiaocai Zhang, Shu Hu, and Wenbin Zhang </div> <div class="periodical"> <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-032-05962-8_20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>As machine learning techniques continue to permeate a variety of application domains with significant societal impact, the focus on algorithmic fairness is becoming an increasingly critical aspect of this established area of research. Existing studies on fairness typically assume that algorithmic bias stems from a single, predefined sensitive attribute in the data, thereby overlooking the reality that multiple sensitive attributes are often prevalent simultaneously in the real world. Unlike previous works, this paper delves into the realm of group fairness involving multiple sensitive attributes, a setting that greatly increases the difficulty of mitigating algorithmic bias. We posit that this multi-attribute perspective provides a more pragmatic model for fairness in real-world applications, and show how learning with such an intricate precondition draws new insights that better explain algorithmic fairness. Furthermore, we develop the first-of-its-kind unified metric, Multi-Fairness Bonded Utility (MFBU), designed to simultaneously evaluate and compare the trade-offs between fairness and utility of multi-source bias mitigation methods. By combining fairness and utility into a single, intuitive metric, MFBU provides model designers the flexibility to holistically evaluate and compare different fairness techniques. Thorough experiments conducted on three real-world datasets substantiate the superior performance of the proposed methodology in minimizing discrimination while maintaining predictive performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yin2025digital" class="col-sm-10"> <div class="title">Digital Forensics in the Age of Large Language Models</div> <div class="author"> <em>Zhipeng Yin</em>, Zichong Wang, Weifeng Xu, Jun Zhuang, Pallab Mozumder, Antoinette Smith, and Wenbin Zhang </div> <div class="periodical"> <em>Artificial Intelligence Driven Forensics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2504.02963" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Digital forensics plays a pivotal role in modern investigative processes, utilizing specialized methods to systematically collect, analyze, and interpret digital evidence for judicial proceedings. However, traditional digital forensic techniques are primarily based on manual labor-intensive processes, which become increasingly insufficient with the rapid growth and complexity of digital data. To this end, Large Language Models (LLMs) have emerged as powerful tools capable of automating and enhancing various digital forensic tasks, significantly transforming the field. Despite the strides made, general practitioners and forensic experts often lack a comprehensive understanding of the capabilities, principles, and limitations of LLM, which limits the full potential of LLM in forensic applications. To fill this gap, this paper aims to provide an accessible and systematic overview of how LLM has revolutionized the digital forensics approach. Specifically, it takes a look at the basic concepts of digital forensics, as well as the evolution of LLM, and emphasizes the superior capabilities of LLM. To connect theory and practice, relevant examples and real-world scenarios are discussed. We also critically analyze the current limitations of applying LLMs to digital forensics, including issues related to illusion, interpretability, bias, and ethical considerations. In addition, this paper outlines the prospects for future research, highlighting the need for effective use of LLMs for transparency, accountability, and robust standardization in the forensic process.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025fg" class="col-sm-10"> <div class="title">Fg-smote: Towards fair node classification with graph neural network</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Yuying Zhang, Liping Yang, Tingting Zhang, Niki Pissinou, Yu Cai, Shu Hu, Yun Li, Liang Zhao, and others </div> <div class="periodical"> <em>ACM SIGKDD Explorations Newsletter</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3715073.3715082" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Graphgenerativemodelshavebecome increasinglyprevalent across various domains due to their superior performance indiverseapplications. However, as theirapplicationrises,particularlyinhigh-riskdecision-makingscenarios, concerns about their fairness are intensifyingwithin the community. Existing graph-basedgenerationmodels mainlyfocusonsynthesizingminoritynodestoenhancethe node classificationperformance. However, byoverlooking thenodegenerationprocess,thisstrategymayintensifyrepresentationaldisparitiesamongdifferentsubgroups,thereby furthercompromisingthefairnessof themodel.Moreover, existingoversamplingmethodsgeneratesamplesbyselectinginstancesfromcorrespondingsubgroups,riskingoverfittinginthosesubgroupsowingtotheirunderrepresentation. Furthermore,theyfailtoaccountfortheinherentimbalance inedgedistributionsamongsubgroups,consequentlyintroducingstructuralbiaswhengeneratinggraphstructure information.Toaddressthesechallenges,thispaperelucidates howexistinggraph-basedsamplingtechniquescanamplify real-worldbiasandproposesanovel framework,FairGraph SyntheticMinorityOversamplingTechnique(FG-SMOTE), aimedatachievinga fairbalance inrepresentingdifferent subgroups. Specifically, FG-SMOTE starts by removing the identifiabilityof subgroup information fromnode representations. Subsequently, the embeddings for simulated nodesaregeneratedbysampling fromthese subgroup informationdesensitizednoderepresentations. Lastly, a fair linkpredictor isemployedtogeneratethegraphstructure information. Extensive experimental evaluations onthree realgraphdatasetsshowthatFG-SMOTEoutperformsthe state-of-the-artbaselines infairnesswhilealsomaintaining competitivepredictiveperformance.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="wang2025graph" class="col-sm-10"> <div class="title">Graph fairness via authentic counterfactuals: Tackling structural and causal challenges</div> <div class="author"> Zichong Wang, <em>Zhipeng Yin</em>, Fang Liu, Zhen Liu, Christine Lisetti, Rui Yu, Shaowei Wang, Jun Liu, Sukumar Ganapati, Shuigeng Zhou, and others </div> <div class="periodical"> <em>ACM SIGKDD Explorations Newsletter</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3715073.3715081" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The extensive use of graph-based Machine Learning (ML) decision-making systems has raised numerous concerns about their potential discrimination, especially in domains with high societal impact. Various fair graph methods have thus been proposed, primarily relying on statistical fairness notions that emphasize sensitive attributes as a primary source of bias, leaving other sources of bias inadequately addressed. Existing works employ counterfactual fairness to tackle this issue from a causal perspective. However, these approaches suffer from two key limitations: they overlook hidden confounders that may affect node features and graph structure, leading to an oversimplification of causality and the inability to generate authentic counterfactual instances; they neglect graph structure bias, resulting in over-correlation of sensitive attributes with node representations. In response, this paper introduces the Authentic Graph Counterfactual Generator (AGCG), a novel framework designed to mitigate graph structure bias through a novel fair message-passing technique and to improve counterfactual sample generation by inferring hidden confounders. Comprising four key modules - subgraph selection, fair node aggregation, hidden confounder identification, and counterfactual instance generation - AGCG offers a holistic approach to advancing graph model fairness in multiple dimensions. Empirical studies conducted on both real and synthetic datasets demonstrate the effectiveness and utility of AGCG in promoting fair graph-based decision-making.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="yin2024fairaied" class="col-sm-10"> <div class="title">FairAIED: Navigating Fairness, Bias, and Ethics in Educational AI Applications</div> <div class="author"> <em>Zhipeng Yin</em>, Sribala Vidyadhari Chinta, Zichong Wang, Matthew Gonzalez, and Wenbin Zhang </div> <div class="periodical"> <em>arXiv preprint arXiv:2407.18745</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2407.18745" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The integration of Artificial Intelligence (AI) into education has transformative potential, providing tailored learning experiences and creative instructional approaches. However, the inherent biases in AI algorithms hinder this improvement by unintentionally perpetuating prejudice against specific demographics, especially in human-centered applications like education. This survey delves deeply into the developing topic of algorithmic fairness in educational contexts, providing a comprehensive evaluation of the diverse literature on fairness, bias, and ethics in AI-driven educational applications. It identifies the common forms of biases, such as data-related, algorithmic, and user-interaction, that fundamentally undermine the accomplishment of fairness in AI teaching aids. By outlining existing techniques for mitigating these biases, ranging from varied data gathering to algorithmic fairness interventions, the survey emphasizes the critical role of ethical considerations and legal frameworks in shaping a more equitable educational environment. Furthermore, it guides readers through the complexities of fairness measurements, methods, and datasets, shedding light on the way to bias reduction. Despite these gains, this survey highlights long-standing issues, such as achieving a balance between fairness and accuracy, as well as the need for diverse datasets. Overcoming these challenges and ensuring the ethical and fair use of AIâ€™s promise in education call for a collaborative, interdisciplinary approach.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="zhang2025datasets" class="col-sm-10"> <div class="title">Datasets for Fairness in Language Models: An In-Depth Survey</div> <div class="author"> Jiale Zhang, Zichong Wang, Avash Palikhe, <em>Zhipeng Yin</em>, and Wenbin Zhang </div> <div class="periodical"> <em>arXiv preprint arXiv:2506.23411</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2506.23411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/stefan0711/Fairness-in-Large-Language-Models/tree/main/datasets" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets to promote transparency and reproducibility across the research community.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="palikhe2025towards" class="col-sm-10"> <div class="title">Towards transparent ai: A survey on explainable language models</div> <div class="author"> Avash Palikhe, Zichong Wang, <em>Zhipeng Yin</em>, Rui Guo, Qiang Duan, Jie Yang, and Wenbin Zhang </div> <div class="periodical"> <em>arXiv preprint arXiv:2509.21631</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2509.21631" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Language Models (LMs) have significantly advanced natural language processing and enabled remarkable progress across diverse domains, yet their black-box nature raises critical concerns about the interpretability of their internal mechanisms and decision-making processes. This lack of transparency is particularly problematic for adoption in high-stakes domains, where stakeholders need to understand the rationale behind model outputs to ensure accountability. On the other hand, while explainable artificial intelligence (XAI) methods have been well studied for non-LMs, they face many limitations when applied to LMs due to their complex architectures, considerable training corpora, and broad generalization abilities. Although various surveys have examined XAI in the context of LMs, they often fail to capture the distinct challenges arising from the architectural diversity and evolving capabilities of these models. To bridge this gap, this survey presents a comprehensive review of XAI techniques with a particular emphasis on LMs, organizing them according to their underlying transformer architectures: encoder-only, decoder-only, and encoder-decoder, and analyzing how methods are adapted to each while assessing their respective strengths and limitations. Furthermore, we evaluate these techniques through the dual lenses of plausibility and faithfulness, offering a structured perspective on their effectiveness. Finally, we identify open research challenges and outline promising future directions, aiming to guide ongoing efforts toward the development of robust, transparent, and interpretable XAI methods for LMs.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div id="yin2024improving" class="col-sm-10"> <div class="title">Improving fairness in machine learning software via counterfactual fairness thinking</div> <div class="author"> <em>Zhipeng Yin</em>, Zichong Wang, and Wenbin Zhang </div> <div class="periodical"> <em>2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3639478.3643531" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Machine Learning (ML) software is increasingly influencing decisions that impact individualsâ€™ lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social subgroups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works enhance software fairness, a non-functional software property, at the cost of software performance. To this end, we proposed a novel framework, namely Group Equality Counterfactual Fairness (GECF), which aims to mitigate sensitive attribute bias and labeling bias using counterfactual fairness while reducing the resulting performance loss based on ensemble learning. Experimental results on 6 real-world datasets show the superiority of our proposed framework from different aspects.</p> </div> </div> </div> </li></ol> <h2 class="bibliography"></h2> <ol class="bibliography"><li> <div class="row"> <div id="palikhedomain" class="col-sm-10"> <div class="title">Domain Knowledge Empowered Language Models: A Survey</div> <div class="author"> Avash Palikhe, Zichong Wang, <em>Zhipeng Yin</em>, Shu Hu, Xuejiao Zhao, Jun Liu, Yu Cai, and Wenbin Zhang </div> <div class="periodical"> <em>Authorea Preprints</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.176403151.11257350" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Language Models (LMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they often fall short in application areas that demand deep, expert-level knowledge for accurate and reliable outcomes. Such domains pose unique challenges that general-purpose LMs are not inherently designed to address, leading to factual inaccuracies and degraded performance with potentially serious consequences. To overcome these limitations, researchers have developed various techniques to equip LMs with domain-specific knowledge for specialized tasks. This survey provides a comprehensive review of domain-specialization techniques for language models across diverse domains, systematically examining their key features, trade-offs, and integration strategies that combine different adaptation methods. It proposes a unified framework that categorizes these methods through a standardized taxonomy based on the domain knowledge integration stage: training phase, inference phase, and hybrid approaches. We further analyze these methods, emphasizing their underlying mechanisms and providing systematic comparisons while discussing their implications for domain-specific adaptation. Finally, we highlight existing challenges and outline promising directions for advancing research in this rapidly evolving field.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Zhipeng Yin. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>