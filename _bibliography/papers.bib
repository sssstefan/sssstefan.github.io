---
---

@article{yin2026fairness,
  abstract={Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their transformer architecture: encoder-only, decoder-only, and encoder-decoder LMs. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The repository is publicly available online at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.},
  title={Fairness Definitions in Language Models Explained},
  author={Yin, Zhipeng and Wang, Zichong and Palikhe, Avash and Zhang, Wenbin},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  year={2026},
  publisher={Wiley Online Library},
  html={https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.70063},
  website={https://github.com/stefan0711/Fairness-in-Large-Language-Models/tree/main/definitions}
}


@article{wang2025unified,
  abstract={Graph generation models play pivotal roles in many real-world applications, from data augmentation to privacy-preserving. Despite their deployment successes, existing approaches often exhibit fairness issues, limiting their adoption in high-risk decision-making applications. Most existing fair graph generation works are based on autoregressive models that suffer from ordering sensitivity, while primarily addressing structural bias and overlooking the critical issue of feature bias. To this end, we propose FairGEM, a novel one-shot graph generation framework designed to mitigate both graph structural bias and node feature bias simultaneously. Furthermore, our theoretical analysis establishes that FairGEM delivers substantially stronger fairness guarantees than existing models while preserving generation quality. Extensive experiments across multiple real-world datasets demonstrate that FairGEM achieves superior performance in both generation quality and fairness.},
  title={A unified framework for fair graph generation: Theoretical guarantees and empirical advances},
  author={Wang, Zichong and Yin, Zhipeng and Zhang, Wenbin},
  journal={The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025},
  html={https://openreview.net/forum?id=T85ADT8a2y}
}

@article{amon2025uncertain,
  abstract={Generative AI is becoming increasingly prevalent in creative f ields, sparking urgent debates over how current copyright laws can keep pace with technological innovation. Recent controversies of AI models generating near-replicas of copyrighted material highlight the need to adapt current legal frameworks and develop technical methods to mitigate copyright infringement risks. This task requires understanding the intersection between computational concepts such as large-scale data scraping and probabilistic content generation, legal definitions of originality and fair use, and economic impacts on intellectual property (IP) rights holders. However, most existing research on copyright in AI takes a purely computer science or law-based approach, leaving a gap in coordinating these approaches that only multidisciplinary efforts can effectively address. To bridge this gap, our survey adopts a comprehensive approach synthesizing insights from law, policy, economics, and computer science. It begins by discussing the foundational goals and considerations that should be applied to copyright in generative AI, followed by methods for detecting and assessing potential violations in AI system outputs. Next, it explores various regulatory options influenced by legal, policy, and economic frameworks to manage and mitigate copyright concerns associated with generative AI and reconcile the interests of IP rights holders with that of generative AI producers. The discussion then introduces techniques to safeguard individual creative works from unauthorized replication, such as watermarking and cryptographic protections. Finally, it describes advanced training strategies designed to prevent AI models from reproducing protected content. In doing so, we highlight key opportunities for action and offer actionable strategies that creators, developers, and policymakers can use in navigating the evolving copyright landscape.},
  title={Uncertain boundaries: Multidisciplinary approaches to copyright issues in generative ai},
  author={Amon, Archer and Yin, Zhipeng and Wang, Zichong and Palikhe, Avash and Yu, Tongjia and Zhang, Wenbin},
  journal={ACM SIGKDD Explorations Newsletter},
  year={2025},
  publisher={ACM New York, NY, USA},
  html={https://arxiv.org/pdf/2404.08221}
}

@article{yin2025amcr,
  abstract={Generative models have achieved impressive results in text to image tasks, significantly advancing visual content creation. However, this progress comes at a cost, as such models rely heavily on large-scale training data and may unintentionally replicate copyrighted elements, creating serious legal and ethical challenges for real-world deployment. To address these concerns, researchers have proposed various strategies to mitigate copyright risks, most of which are prompt based methods that filter or rewrite user inputs to prevent explicit infringement. While effective in handling obvious cases, these approaches often fall short in more subtle situations, where seemingly benign prompts can still lead to infringing outputs. To address these limitations, this paper introduces Assessing and Mitigating Copyright Risks (AMCR), a comprehensive framework which i) builds upon prompt-based strategies by systematically restructuring risky prompts into safe and non-sensitive forms, ii) detects partial infringements through attention-based similarity analysis, and iii) adaptively mitigates risks during generation to reduce copyright violations without compromising image quality. Extensive experiments validate the effectiveness of AMCR in revealing and mitigating latent copyright risks, offering practical insights and benchmarks for the safer deployment of generative models.},
  title={AMCR: A framework for assessing and mitigating copyright risks in generative models},
  author={Yin, Zhipeng and Wang, Zichong and Palikhe, Avash and Liu, Zhen and Liu, Jun and Zhang, Wenbin},
  journal={28th European Conference on Artificial Intelligence (ECAI)},
  year={2025},
  html={https://arxiv.org/abs/2509.00641}
}

@article{wang2025ai,
  abstract={Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.},
  title={AI fairness beyond complete demographics: Current achievements and future directions},
  author={Wang, Zichong and Yin, Zhipeng and Yap, Roland HC and Zhang, Wenbin},
  journal={28th European Conference on Artificial Intelligence (ECAI)},
  year={2025},
  html={https://arxiv.org/abs/2511.13525}
}

@article{wang2025towards,
  abstract={Machine Learning (ML) software is increasingly influencing decisions that impact individuals’ lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social groups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works focus on tabular data. Exploration in the context of graph data, which is widely prevalent in real-world applications, has been relatively neglected. This paper thus sheds light on the impact of biases within graph data on the fairness of ML models and how to detect biases in graph based software. Subsequently, we introduce Fair Graph Redistribution and Generation (FGRG), a novel fair imbalance node learning technique designed to enhance the fairness of graph-based ML software. Specifically, FGRG commences with the generation of unbiased node embeddings for each node. Next, FGRG identifies node similarities within the embedding space and generates new nodes to rebalance the internal distribution, ensuring subgroups with different sensitive attributes are balanced representations across both positive and negative classes. Experiments on 8 real-world graphs across 3 fairness and 4 performance measurements show that our framework significantly outperforms the state-of-the-art baselines, and also achieves comparable prediction performance. In particular, FGRG beats the trade-off baseline in 79.16% of the fairness cases evaluated and 62.5% of the performance cases evaluated.},
  title={Towards fair graph-based machine learning software: unveiling and mitigating graph model bias},
  author={Wang, Zichong and Yin, Zhipeng and Zhang, Xingyu and Zhang, Yuying and He, Xudong and Wang, Shaowei and Song, Houbing and Yang, Liping and Zhang, Wenbin},
  journal={AI and Ethics},
  year={2025},
  publisher={Springer},
  html={https://link.springer.com/article/10.1007/s43681-025-00756-y}
}

@article{wang2025fairness,
  award={Best student paper award},
  award_name={Best student paper award},
  abstract={Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node’s contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework’s effectiveness in both mitigating bias and maintaining model utility.},
  title={Fairness-aware graph representation learning with limited demographic information},
  author={Wang, Zichong and Yin, Zhipeng and Yang, Liping and Zhuang, Jun and Yu, Rui and Kong, Qingzhao and Zhang, Wenbin},
  journal={Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)},
  year={2025},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-032-05962-8_21}
}

@article{wang2025redefining,
  abstract={As machine learning techniques continue to permeate a variety of application domains with significant societal impact, the focus on algorithmic fairness is becoming an increasingly critical aspect of this established area of research. Existing studies on fairness typically assume that algorithmic bias stems from a single, predefined sensitive attribute in the data, thereby overlooking the reality that multiple sensitive attributes are often prevalent simultaneously in the real world. Unlike previous works, this paper delves into the realm of group fairness involving multiple sensitive attributes, a setting that greatly increases the difficulty of mitigating algorithmic bias. We posit that this multi-attribute perspective provides a more pragmatic model for fairness in real-world applications, and show how learning with such an intricate precondition draws new insights that better explain algorithmic fairness. Furthermore, we develop the first-of-its-kind unified metric, Multi-Fairness Bonded Utility (MFBU), designed to simultaneously evaluate and compare the trade-offs between fairness and utility of multi-source bias mitigation methods. By combining fairness and utility into a single, intuitive metric, MFBU provides model designers the flexibility to holistically evaluate and compare different fairness techniques. Thorough experiments conducted on three real-world datasets substantiate the superior performance of the proposed methodology in minimizing discrimination while maintaining predictive performance.},
  title={Redefining fairness: A multi-dimensional perspective and integrated evaluation framework},
  author={Wang, Zichong and Yin, Zhipeng and Liu, Zhen and Yap, Roland HC and Zhang, Xiaocai and Hu, Shu and Zhang, Wenbin},
  journal={Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD)},
  year={2025},
  organization={Springer},
  html={https://link.springer.com/chapter/10.1007/978-3-032-05962-8_20}
}

@article{yin2025digital,
  abstract={Digital forensics plays a pivotal role in modern investigative processes, utilizing specialized methods to systematically collect, analyze, and interpret digital evidence for judicial proceedings. However, traditional digital forensic techniques are primarily based on manual labor-intensive processes, which become increasingly insufficient with the rapid growth and complexity of digital data. To this end, Large Language Models (LLMs) have emerged as powerful tools capable of automating and enhancing various digital forensic tasks, significantly transforming the field. Despite the strides made, general practitioners and forensic experts often lack a comprehensive understanding of the capabilities, principles, and limitations of LLM, which limits the full potential of LLM in forensic applications. To fill this gap, this paper aims to provide an accessible and systematic overview of how LLM has revolutionized the digital forensics approach. Specifically, it takes a look at the basic concepts of digital forensics, as well as the evolution of LLM, and emphasizes the superior capabilities of LLM. To connect theory and practice, relevant examples and real-world scenarios are discussed. We also critically analyze the current limitations of applying LLMs to digital forensics, including issues related to illusion, interpretability, bias, and ethical considerations. In addition, this paper outlines the prospects for future research, highlighting the need for effective use of LLMs for transparency, accountability, and robust standardization in the forensic process.},
  title={Digital Forensics in the Age of Large Language Models},
  author={Yin, Zhipeng and Wang, Zichong and Xu, Weifeng and Zhuang, Jun and Mozumder, Pallab and Smith, Antoinette and Zhang, Wenbin},
  journal={Artificial Intelligence Driven Forensics},
  year={2025},
  html={https://arxiv.org/abs/2504.02963}
}

@article{wang2025fg,
  abstract={Graphgenerativemodelshavebecome increasinglyprevalent across various domains due to their superior performance indiverseapplications. However, as theirapplicationrises,particularlyinhigh-riskdecision-makingscenarios, concerns about their fairness are intensifyingwithin the community. Existing graph-basedgenerationmodels mainlyfocusonsynthesizingminoritynodestoenhancethe node classificationperformance. However, byoverlooking thenodegenerationprocess,thisstrategymayintensifyrepresentationaldisparitiesamongdifferentsubgroups,thereby furthercompromisingthefairnessof themodel.Moreover, existingoversamplingmethodsgeneratesamplesbyselectinginstancesfromcorrespondingsubgroups,riskingoverfittinginthosesubgroupsowingtotheirunderrepresentation. Furthermore,theyfailtoaccountfortheinherentimbalance inedgedistributionsamongsubgroups,consequentlyintroducingstructuralbiaswhengeneratinggraphstructure information.Toaddressthesechallenges,thispaperelucidates howexistinggraph-basedsamplingtechniquescanamplify real-worldbiasandproposesanovel framework,FairGraph SyntheticMinorityOversamplingTechnique(FG-SMOTE), aimedatachievinga fairbalance inrepresentingdifferent subgroups. Specifically, FG-SMOTE starts by removing the identifiabilityof subgroup information fromnode representations. Subsequently, the embeddings for simulated nodesaregeneratedbysampling fromthese subgroup informationdesensitizednoderepresentations. Lastly, a fair linkpredictor isemployedtogeneratethegraphstructure information. Extensive experimental evaluations onthree realgraphdatasetsshowthatFG-SMOTEoutperformsthe state-of-the-artbaselines infairnesswhilealsomaintaining competitivepredictiveperformance.},
  title={Fg-smote: Towards fair node classification with graph neural network},
  author={Wang, Zichong and Yin, Zhipeng and Zhang, Yuying and Yang, Liping and Zhang, Tingting and Pissinou, Niki and Cai, Yu and Hu, Shu and Li, Yun and Zhao, Liang and others},
  journal={ACM SIGKDD Explorations Newsletter},
  year={2025},
  publisher={ACM New York, NY, USA},
  html={https://dl.acm.org/doi/abs/10.1145/3715073.3715082}
}

@article{wang2025graph,
  abstract={The extensive use of graph-based Machine Learning (ML) decision-making systems has raised numerous concerns about their potential discrimination, especially in domains with high societal impact. Various fair graph methods have thus been proposed, primarily relying on statistical fairness notions that emphasize sensitive attributes as a primary source of bias, leaving other sources of bias inadequately addressed. Existing works employ counterfactual fairness to tackle this issue from a causal perspective. However, these approaches suffer from two key limitations: they overlook hidden confounders that may affect node features and graph structure, leading to an oversimplification of causality and the inability to generate authentic counterfactual instances; they neglect graph structure bias, resulting in over-correlation of sensitive attributes with node representations. In response, this paper introduces the Authentic Graph Counterfactual Generator (AGCG), a novel framework designed to mitigate graph structure bias through a novel fair message-passing technique and to improve counterfactual sample generation by inferring hidden confounders. Comprising four key modules - subgraph selection, fair node aggregation, hidden confounder identification, and counterfactual instance generation - AGCG offers a holistic approach to advancing graph model fairness in multiple dimensions. Empirical studies conducted on both real and synthetic datasets demonstrate the effectiveness and utility of AGCG in promoting fair graph-based decision-making.},
  title={Graph fairness via authentic counterfactuals: Tackling structural and causal challenges},
  author={Wang, Zichong and Yin, Zhipeng and Liu, Fang and Liu, Zhen and Lisetti, Christine and Yu, Rui and Wang, Shaowei and Liu, Jun and Ganapati, Sukumar and Zhou, Shuigeng and others},
  journal={ACM SIGKDD Explorations Newsletter},
  year={2025},
  publisher={ACM New York, NY, USA},
  html={https://dl.acm.org/doi/pdf/10.1145/3715073.3715081}
}

@article{yin2024improving,
  abstract={Machine Learning (ML) software is increasingly influencing decisions that impact individuals' lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social subgroups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works enhance software fairness, a non-functional software property, at the cost of software performance. To this end, we proposed a novel framework, namely Group Equality Counterfactual Fairness (GECF), which aims to mitigate sensitive attribute bias and labeling bias using counterfactual fairness while reducing the resulting performance loss based on ensemble learning. Experimental results on 6 real-world datasets show the superiority of our proposed framework from different aspects.},
  title={Improving fairness in machine learning software via counterfactual fairness thinking},
  author={Yin, Zhipeng and Wang, Zichong and Zhang, Wenbin},
  journal={2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
  year={2024},
  html={https://dl.acm.org/doi/abs/10.1145/3639478.3643531}
}

@article{yin2024fairaied,
  abstract={The integration of Artificial Intelligence (AI) into education has transformative potential, providing tailored learning experiences and creative instructional approaches. However, the inherent biases in AI algorithms hinder this improvement by unintentionally perpetuating prejudice against specific demographics, especially in human-centered applications like education. This survey delves deeply into the developing topic of algorithmic fairness in educational contexts, providing a comprehensive evaluation of the diverse literature on fairness, bias, and ethics in AI-driven educational applications. It identifies the common forms of biases, such as data-related, algorithmic, and user-interaction, that fundamentally undermine the accomplishment of fairness in AI teaching aids. By outlining existing techniques for mitigating these biases, ranging from varied data gathering to algorithmic fairness interventions, the survey emphasizes the critical role of ethical considerations and legal frameworks in shaping a more equitable educational environment. Furthermore, it guides readers through the complexities of fairness measurements, methods, and datasets, shedding light on the way to bias reduction. Despite these gains, this survey highlights long-standing issues, such as achieving a balance between fairness and accuracy, as well as the need for diverse datasets. Overcoming these challenges and ensuring the ethical and fair use of AI's promise in education call for a collaborative, interdisciplinary approach.},
  title={FairAIED: Navigating Fairness, Bias, and Ethics in Educational AI Applications},
  author={Yin, Zhipeng and Chinta, Sribala Vidyadhari and Wang, Zichong and Gonzalez, Matthew and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2407.18745},
  year={2025},
  html={https://arxiv.org/pdf/2407.18745}
}

@article{zhang2025datasets,
  abstract={Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets to promote transparency and reproducibility across the research community.},
  title={Datasets for Fairness in Language Models: An In-Depth Survey},
  author={Zhang, Jiale and Wang, Zichong and Palikhe, Avash and Yin, Zhipeng and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2506.23411},
  year={2025},
  html={https://arxiv.org/abs/2506.23411},
  website={https://github.com/stefan0711/Fairness-in-Large-Language-Models/tree/main/datasets}
}

@article{palikhe2025towards,
  abstract={Language Models (LMs) have significantly advanced natural language processing and enabled remarkable progress across diverse domains, yet their black-box nature raises critical concerns about the interpretability of their internal mechanisms and decision-making processes. This lack of transparency is particularly problematic for adoption in high-stakes domains, where stakeholders need to understand the rationale behind model outputs to ensure accountability. On the other hand, while explainable artificial intelligence (XAI) methods have been well studied for non-LMs, they face many limitations when applied to LMs due to their complex architectures, considerable training corpora, and broad generalization abilities. Although various surveys have examined XAI in the context of LMs, they often fail to capture the distinct challenges arising from the architectural diversity and evolving capabilities of these models. To bridge this gap, this survey presents a comprehensive review of XAI techniques with a particular emphasis on LMs, organizing them according to their underlying transformer architectures: encoder-only, decoder-only, and encoder-decoder, and analyzing how methods are adapted to each while assessing their respective strengths and limitations. Furthermore, we evaluate these techniques through the dual lenses of plausibility and faithfulness, offering a structured perspective on their effectiveness. Finally, we identify open research challenges and outline promising future directions, aiming to guide ongoing efforts toward the development of robust, transparent, and interpretable XAI methods for LMs.},
  title={Towards transparent ai: A survey on explainable language models},
  author={Palikhe, Avash and Wang, Zichong and Yin, Zhipeng and Guo, Rui and Duan, Qiang and Yang, Jie and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2509.21631},
  year={2025},
  html={https://arxiv.org/abs/2509.21631}
}

@article{palikhedomain,
  abstract={Language Models (LMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they often fall short in application areas that demand deep, expert-level knowledge for accurate and reliable outcomes. Such domains pose unique challenges that general-purpose LMs are not inherently designed to address, leading to factual inaccuracies and degraded performance with potentially serious consequences. To overcome these limitations, researchers have developed various techniques to equip LMs with domain-specific knowledge for specialized tasks. This survey provides a comprehensive review of domain-specialization techniques for language models across diverse domains, systematically examining their key features, trade-offs, and integration strategies that combine different adaptation methods. It proposes a unified framework that categorizes these methods through a standardized taxonomy based on the domain knowledge integration stage: training phase, inference phase, and hybrid approaches. We further analyze these methods, emphasizing their underlying mechanisms and providing systematic comparisons while discussing their implications for domain-specific adaptation. Finally, we highlight existing challenges and outline promising directions for advancing research in this rapidly evolving field.},
  title={Domain Knowledge Empowered Language Models: A Survey},
  author={Palikhe, Avash and Wang, Zichong and Yin, Zhipeng and Hu, Shu and Zhao, Xuejiao and Liu, Jun and Cai, Yu and Zhang, Wenbin},
  journal={Authorea Preprints},
  publisher={Authorea},
  html={https://www.techrxiv.org/doi/full/10.36227/techrxiv.176403151.11257350}
}





